{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deep Learning Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras import Input\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "import visualkeras\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a model\n",
    "\n",
    "- Create a model as a sequential model\n",
    "- Defining the number of neuron in each layer and activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 23:50:33.872895: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameter\n",
    "num_classes = 10\n",
    "input_shape = (784)\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=input_shape),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAE0AAAA/CAYAAACl40V6AAAESklEQVR4nO2b7W9TVRzHv5eHdQGyKa1jVKZjEMEalfCQ4LqHMKLFCMQgkIXgG4kmvJA/QB2pCSGGGHlhGEwx6OJebIsEhqOD4IBmOMyGLNnaDHWjbMtct2k3XOtW111fLHe2d30499fb9naez6u259z7/d1Pz72nvacVRFEUQeT8udOoqPgQhS/nUHcRwj3nKPp+98JcVASj0Rix3+hQL5wOBwo3qZfbP+RFQ0MD3ti9J2b/JdSg8+dO42PrcVyrfBXr87Kou5nj2Km7mPIHoH8yE1VVVTCZTFFyr+LaWXVzVz6RibUF65i2WUQJkoQ1fl6mWuGN9gFc/qwE+uxMzeXKUSwtkYUXrFmhudxwKJLGhc3CLI0L+w+BZfY8dvQwrjZewTLdYmQsJV0GQ5gOiPi1bxyvvGjA8mWhc9HNtmEUFpchKysLfT0dGB1xq5o79tcU6j4xzxNWdKQZFxvvRJyAgok5e/r9fjweH8PmjQYceK2AXnEQ9dd7sXgRcOj1Z+e1tTs9sFgsyM3NRc2FR3hGH1A11zc5RR5hEjGlZWRkID9/LZA9pFrxjh4PfJN+7C1dM6/tVPUvsFgsMJlMeND5I+BpVTW33eGOez/xj/n/IVwaAS6NAJdGgEsjwKUR4NIIcGkEuDQCXBoB8p3bRDHlD6CpqQldXV1wuVzIz05O7swM+11/TUmrsbkw7p2G3W6HTqfD8IAL+S/pkpL72BdATg7bmoNmpNXYXPj0217c/elnPLdhIwDA+sH7gKc1Kbn2ljYYDAambTRxTZMK/+FW65wwLeemXFq6CQNSLC0dhQEplDYw7EuJMDVymSaCiYkJ2K73wtnjIYXIcfR40O/2wly8Ax9VHI/Yb9DVCc8fblVzRQhxv1FMCyucUFI+EaQjXBoBLo0Al0aASyPApRHg0ghwaQS4NAKk+2nNN2w4sH8fjry5HkuXRPfe3DaI+91/4u3y3fjym8ukIrWGYmnNN2woP7gfNSeLUbJ5VdS+Z2qd6H44+zu01aufJhepNRSdnpKw6hNmJmEnv+rE19bt2PaCPq4itQazNKow86an4i5SazCfnu+9U451eStQWdeNyrruiP18f0+j8zfPghUGKJCWt2o5SrdG/heJxO32QWx5Xk8WJghCyHMt3rlilla61Qjr0S0x+1nPQpWfaIqiCEEQIAjCnLhgoVJ7uMcSkd4A+X6Ukjaf0+RS5DLlQuRi5O3hBLKSNtIkWA9SPuLk21FkSWhmsZgVJadTrNFHJSnS/pmeAfvftWaRn0bB1ziJaAcebiQFS4xHYMKltXSMoPp7Fy5deYupf7QDCNcW/Fqkx0oyWEjoNa2lYwTvnriH+ouXUFyyM5FRSSVh0iRhtfXfoWznrkTFpISESBsdm1ywwgAFi8U7thmZvxH0u7344kLtghQG8BV2Emn34VYLcGkEuDQCXBoBLo0Al0aASyPApRHg0gj8C8pFLQqCVoCPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=77x63 at 0x7FC6F7D6C9D0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualkeras.layered_view(model, legend=True, draw_volume=True, spacing=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model parameters\n",
    "\n",
    "- 1<sup>st</sup> layer: (784 + 1) * 32 = 25,120\n",
    "- 2<sup>nd</sup> layer: (32 + 1) * 10 = 330\n",
    "- Total parameter: 25,450\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compile a model\n",
    "\n",
    "- Setting optimizer\n",
    "  - `adam`, `SGD`, `rmsprop`, etc\n",
    "- Setting loss function\n",
    "  - `mean_squared_error`, `mean_absolute_error`, `categorical_crossentropy`, etc\n",
    "- Setting evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training a model\n",
    "\n",
    "- Load MNIST data\n",
    "- Traning data (60k) + Testing data (10k)\n",
    "- Convert from 28x28 pixel to 784 one dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 784)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "# Convert from (28, 28) to (784)\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "# Make sure images have shape (784)\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 00:01:54.993679: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 1s 1ms/step - loss: 0.5260 - accuracy: 0.8551 - val_loss: 0.2355 - val_accuracy: 0.9348\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 0s 908us/step - loss: 0.2538 - accuracy: 0.9291 - val_loss: 0.1828 - val_accuracy: 0.9512\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 0s 903us/step - loss: 0.2079 - accuracy: 0.9413 - val_loss: 0.1543 - val_accuracy: 0.9588\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 0s 908us/step - loss: 0.1787 - accuracy: 0.9483 - val_loss: 0.1396 - val_accuracy: 0.9623\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 0s 912us/step - loss: 0.1583 - accuracy: 0.9539 - val_loss: 0.1255 - val_accuracy: 0.9667\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 0s 910us/step - loss: 0.1416 - accuracy: 0.9592 - val_loss: 0.1202 - val_accuracy: 0.9647\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9628 - val_loss: 0.1160 - val_accuracy: 0.9685\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 0s 917us/step - loss: 0.1173 - accuracy: 0.9661 - val_loss: 0.1085 - val_accuracy: 0.9677\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 0s 945us/step - loss: 0.1075 - accuracy: 0.9686 - val_loss: 0.1061 - val_accuracy: 0.9675\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 0s 938us/step - loss: 0.0998 - accuracy: 0.9709 - val_loss: 0.1044 - val_accuracy: 0.9707\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 0s 918us/step - loss: 0.0929 - accuracy: 0.9731 - val_loss: 0.1021 - val_accuracy: 0.9710\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 0s 937us/step - loss: 0.0868 - accuracy: 0.9748 - val_loss: 0.1008 - val_accuracy: 0.9702\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 0s 931us/step - loss: 0.0814 - accuracy: 0.9768 - val_loss: 0.0986 - val_accuracy: 0.9713\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 0s 929us/step - loss: 0.0771 - accuracy: 0.9778 - val_loss: 0.1000 - val_accuracy: 0.9707\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9788 - val_loss: 0.1003 - val_accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6e940e1f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=1, validation_split=.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10482573509216309\n",
      "Test accuracy: 0.9681000113487244\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neuron Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                51264     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 61,482\n",
      "Trainable params: 61,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "cnn_model = Sequential(\n",
    "    [\n",
    "        Input(shape=input_shape),\n",
    "        Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAADJCAYAAAB2QWTOAAAZJklEQVR4nO3deXgTdeLH8U/SC2ig0JbTdrmUo1xqAUUoR0EQ8QALXsshqLseq6u4/vRZQWBVRN1FluXyQMQfh8ghVwEVaimVlquw2JYCLdJC7wtK6ZE0yf7RDVtKjkkySebbfF7P4/MAmXy/46R5MxlmMiqj0WiEIL5c9Snmzn0H9w1oJ8t4J9JLkJ1/HStXrsALL7woy5hE5Fq+nl4Bqb5c9SkWzH8XP6y4H7eHt3J6vFc/TkatVg9fHxVat24jwxoSkTuoPb0CUpiCFfuvaNmCFZtwGTsWD4efnw9qampkWEsicgfFR8uVweoWpoFaBVRXV8uwpkTkDoqOlquDBQAqlQq1tbVOj01E7qHYaLkjWACgVoEfD4kEoshouStYAKBSq/jxkEggiouWO4MFAGqVintaRAJRVLTcHSwAUPHjIZFQFBMtTwQLqN/T4sdDInEoIlqeChZQv6fFfz0kEofHo+XJYAGAWs1jWkQi8Wi0PB0soP48LX48JBKHx6KlhGABPE+LSDQeiZZSggXU72kxWkTicHu0lBQsgMe0iETj1mgpLVgAz9MiEo3boqXEYAE8I55ING6JllKDBXBPi0g0Lo+WkoMF1B/T4ikPROJwabSUHiyA/3pIJBqXRUuEYAE8T4tINC6JlijBAur3tFQqFXQ6nazjEpFryB4tkYJl0qxZM1RVVblkbCKSl6zREjFYAKNFJBLZoiVqsABGi0gkskRL5GABQPPmzRktIkE4HS3RgwVwT4tIJE5FqykEC6iPFk8wJRKDw9FqKsECuKdFJBKHotWUggXwmBaRSOyOVlMLFgAEBAQwWkSCsCtaTTFYAPe0iEQiOVpNNViAa45plZaWyjoeEdVTGY1Go62FXn1xKvbE7kKLAB/4+zl/apeuzoCC0irs+nSEx4M17Nk49OwSidJLeWjXvp0sYxYVFKBEW40TGekIDAyUZUwiqudrawGtVouKq1dwd69QTBnbTZZJv9l5DudzKrDnlzz86YkesozpsCrgaFIy3ukyGP5lzgd5dW4qjl4rRHj7jhg6dChWrlyJIUOGyLCiRARIiJa/vz+6dOkKBBXIFq20rHJU1WqxcV82yiu0mPNcH6hUKlnGtse6Ldm4ekWLLf0fQtfmQU6P99a5BFysqcAjwV3Q95EH0S86ClOmTMGECRPw4YcfIjg4WIa1JvJuHrvvYfMAX+xcMhyHThbjzSWnoNfb/JQqq3VbsvHJmgxZg/VjWQ7W9xiDzgEtoVKp8OSTTyItLQ0BAQHo06cP1q5dCwmfxonICo/eYTokKADb/j4MF3Ir8ccPjkKrM7hlXlOwvus7QfZgdWl28z9SBAUFYenSpdi1axeWLVuG6OhopKenOz2nOQwieQObHw9dTdPCDxsW3oc/vn8UU+ccxpr59yKwuetWy53BamjgwIFITk7GqlWrMHLkSEx89CGMiR7h9PwmCQcPYPCQaEyf8YxsYxIpkcejBQDN/H2wet49mL34JKa8lYgNH9yH1i39ZZ/HU8Ey8fHxwcsvvwxtVSk+/mgRSn+Lc3odAOBEegmy868jKDhMlvGIlEwR0QIAXx81lrxxN+Z/9isenZ2A7xYNQ/uQZrKN7+lgmXy56lMs/scn+GHl/bKc7/bKR8mo1eqhaeGHw0lJTo9HpHQePabVmFqtwoIX+mHiyDA89NpBXMy7Lsu4SgqWnCfovvJRMvYcqj9Bt2NoC6SkpKCgoMDpcYmUTFHRAupvNPH673vhpSl34NHZCTjz21WnxvOGYHUL00CtVmH06NFYv36902MTKZniomUy85FumPeHvoh5MxHH08scGsNbgmXy6KOP8rQKavIUGy0AeCw6HEvfjMS0uUmIP1Fo13O9LVgAEBkZievXryMlJcXpeYiUStHRAoAx93TAmvn34KWFx7ErIVfSc7wxWACgVqsxffp0rF271um5iJRK8dECgHv7hWLToqH467J/Y92ei1aXVUqwzl9MdWuwTKZPn46NGzeitrbW6TmJlEiIaAFAvztaY8fi4ViyIQPLN50zu4xSgpXtV4HEk8fcHiwA6Nq1K/r164fdu3c7PS+REgkTLQDoFqbBzk9HYOMP2Xj/y9SbDjgrJVhbazJxvKoYe5ePcXuwTGbMmMGPiNRkCRUtAOjUtjl2fHrzhdZKCtayol+xd4XnggUAMTExOHToEM/ZoiZJMWfE2yMkKABbPxmGGfOSMeHZBGTmXcPYkM5Yn5/h9NgnK4qQUVWG73qOdShYsR7cwzLRaDSYNGkS1q9fjzfeeMPpdSFSEiGjBdRfaP3ei/3x9KtHENm6I3RqIF/v/L0LT10vRnO1L546tx93a9pioKYtIjVtEdEiGH4q8zumSgqWyYwZM/DKK69g9uzZHvmuMiJXETZaABDRLQiaQF980DMKvTQhsow5KPEbvNmhP3q3aIOUymIcryzG96UXcKm2Ev0DQxCpaYtITTvcGRgKjY+fIoMFAFFRUTfO2YqMjHR6vYiUQuhouVJH/0BMCA7EhOAuAICKOi1OXi/BicoirMxPRVpVGbp01KBGbUCboADMmveL03Pq6gy4XHQdO/7h/M0+Gp6zJXe0jEYj997cwFPbWenzMloStfL1x4igThgR1AkAcL1OizlVR9C8i69sX0O9PvY8CkurMe3dJMz/Yz88OLSjUz8806dPxz13RWJw3wHw9ZPnpd6zYxda3dYBw0cMt7pcbkkRWrZpLcuc3mhH7C6Et26LkcNtbOeiIrQMaiPfvLt2I7xdiNvn/SkuHuNHReGZ6dNtLstoOSjQ1x93NGuNVt39ZP3u/Dq9Hs883BnzVv6K1duz8N6L/dGnu2P/Irp/6w6oqmvx3TsLZVm/UxVFuKStRNSwYSguKba4XHrBJaSmpwF33S7LvF4nNRvIL8WwqGEoLba8nc9k5yI1LQ2qzr1kmdZwOQu4Woxhwzwz77SYiZKWZ7QUaGRkexz4rC3W7bmIx99OxANDOuLtmRFo20b694t98fclWPDuu9gm43fg1xr1CPFrjlWffYaIiAizy324ahm2LpgHfD4b+J08t2TzKgs3ADodVG1a4rNVlrfzoqXLse3H+fCdOQ/qkI5OT6vd9QWg10EV2AqfWXl9XTlvt25dJT1HuPO0vIWvjxrPPNwNh9fcD00LP0Q9ux/LN51DrVZv87mmYG3qM17289aC/S2H88NVyzB3wTzoV7zKYDli4QYg4TSw9EWoW1s+prlo6XLMnTcfqmnvyBYO49kTwFNvQd2ipeLmbYzRUrggjT8WvNAPsUtH4EhqKYY9ux+xiXkWv37GlcGydt4ag+WkBsFCWFuLi7kyHGjTXnHzmsNoCaJ7WEt8894QfPLanfj463TEvJmItKybvyDx+6/XMVgiYrDswmgJpv54VzQeGXEbHn87EW8sTkFxeQ1QBaxYtozBEg2DZTceiBeQ6XjXpFFhWLzuLCY8kwjoVWjt44+Xz/7s9Ph6oxFZVVcwSNMWi/P+fdNj+TWVmD17Nlq1aoVfLpxBXlER0LIFMGeN0/N6Hb0ByC4EBnQFPt9700OGovIb2/lw6jnkFxUCzQKBbctg+6imDQY9jCX5QFgPIGHrzQ9VlLl23urrTgULYLSEFqTxxzvP9kFhlg51uX6Y2OEOWcbdXnAeagCTQ7vf8tjJqlKMGzcOHTp0QOqaQhSENof6gcGyzOttDPuOwqBWAQ/euv1U6Tk3tnN63uco8tPAv/8wWebVnk6EEWoY+906nir/gkvnRU0NDE4EC2C0hOfvp0a3cA3UVwNli1ZGZSmqdbUY36bzLY/9qzAN48aNQ0REBH5MPYFzV7OgHjdIlnm9jTEzF6ipBUYNuOUx1df7b2znn46eRFbaZfj3HyrLvPrCHOhra2Dsdevrpkra6dp5c8x/F549eEyLiITCaBGRUBgtIhIKo0VEQmG0iEgojBYRCYXRIiKhMFpEJBRGi4iEwmgRkVB4GQ/ZRavXY9++fUhNTcXFixcB+b4mnBowaHU3b2c3vVUNdXUemdfS98OZw2iRZJtLMlFh0CEhIQEBAQHIvpwNtAnz9Go1PbFHoa6s+d92zs4GzFy8LrvTh6CurfLIvD7aarRrJ+2rjRgtkmRzSSaWlZ9D8qkU9Ohdf0ODme/8Bf9/NcvDa9bExB5Fi7VxOH70OHr36AkAmPX6/2Fj2mXXznv6EFoci8XxE8fQu5cH5j2ShNDQUElP4TEtsskUrJ+TD98IFrmAKVgHE28Eyy1M4UhMuBEsJc/LaJFVDJabMFiSMVpkEYPlJgyWXRgtMitPe53BcofCcs8Eq6LMM8GSYV4eiG8CLhVU4WRBHjIqy2QZL6OyFHm1lRg6ehTmzHvX4nJJ2edgKC2u/wZOspsxMxeqgnJERY3AvDlzLS6XfCYT2uIS6AtzZJlXX5gD45USRI0cgXnvundeH8DpUDJagvv5WCHiMnIBqHG+uQFz5s6Bj9rHqTEPHjiAVh3a4e7B1r9GedDlHLQMDYGfn79T83mrnw7GITwoFIMjI60uNyjnEloGh8Dfz0+eeX+OR1hoMAYPdP+8jz/6iNN7doyWwFIzr+DlRcfx1PjO2PDDZWzYvhUDBtz6feP2mjJjKlQqlQxrSNbMeuIpj2znmb8Xe15GS1C5RVWYOjcJE6I6YVt8EX5JOoEePeU59sRguYentrPo8/JAfGPSrybwmIpKHZ7+62EMjAjG/mPlOBCfJFuwiJSOe1qNKXwnQ6szYNaCIwhpHYDjZyoRdzCZwSKvwmgJxGg0YvbiFJRW1KL8Ghgs8kqMlkA++eYMkn8tgc7gh58ZLPJSPKYliA17L2LNzt+grfNlsMirMVoC+PlYIeauPA0fX3/EHzrCYJFXY7SckF1zzeVzpGZeway/HUFAQDMkJB5lsMjrMVoO2lySiTPaCpfOkVtUhUl/OQR/f38kHj7GYBGB0XKI6dsPHpjwoMvmqKjUYfwr8TDCB0nJ8p04SiQ6RstODb+upVVQkEvm0OoMGP9qPCqqDDhyNIXBImqApzzYwR3fL2U0GjHxjQTkFFQjJeUUevbq7ZJ5iETFaEnkri/EO3GmFOUVOhw9egy9I/q4bB4iUTFaEpgLVllZGU6dOoXzZy8gPatclnlSzpSgvEKL+PgEDLjzLlnGJGpqGC0bGgcrNzcXS5YswVdffYXoUSPw+lt/Q1CrVrLM1S4+DmPGPYQh9w2VZTyipojRsqJhsOCjxvPPP49t27ZhxowZOHXqFMLDw2Wdb/IT0/i1MEQ2MFoWmIL1r9VfYM68dxEfH4+XXnoJZ8+elXx/NnsxWES2MVpm7C3PwaGaYvS6awD+/PprmD17Nr766itoNBpPrxqR1xM+WlqdAQdKspFRWSrLeGXaauyuvorwzp0xa9YsTJ06Ff7+/A50IqUQOlqXCqvgp1YjUf0bAmD/zRzKrmqRX1qDiN594PffMAW30OC111/BX+e8Ax8f524QQUTyEzpaH32djocndMDbMyPsfu76vRfx93UXcODYzV9VXFZWhuDgYDlXk4hkJGy00i9cRdyxQhxZO9bu594IlpnvVmewiJRN2GsPP1idhtee7omWgfbdk81asIhI+YSM1uF/F+NsdgVmPNTVrucxWETiEy5aRqMR732RhreeiUCAv/QD5QwWUdMgXLT2JOahRqdHTLT0s9EZLKKmQ6ho1ekN+OCrdMx9ri/UamlnjzNYRE2LUNHauC8bHUKaYdTAdpKWZ7CImh5holVVU4dPvjmDuc/3lXSNHoNF1DQJE60vvs/C4D4huKtnG5vLMlhETZcQJ5eWVdRi5ebziF06wuayDBZR0ybEntY/N5zDw8NvQ/ewllaXY7CImj7F72ldKqzCtz9kI+HLMVaXY7CIvIPi97Q+XpuOmY90Q/uQZhaXYbCIvIei97TSL1zFgaPWL4pmsIi8i6L3tGxdFM1gEXkfxUbL1kXRDBaRd1JktEwXRb890/xF0QwWkfdSZLT2/JKPGp0ej4269aJoBovIuykuWnV6Az5YnWb2omgGi4gUFy1LF0UzWEQEKCxali6KZrCIyERR0TJ3UTSDRUQNKebk0rKKWqzakonYf/7vomgGi4gaU8ye1j831l8U3S2s/tbzDBYRmaOIPa1LhVX4dl82Dq2uvyiawSIiSxSxp2W6KLpdcDMGi4is8vieVsOLohksIrLF43tapouidybkMlhEZJNHo5V0ugRnsysQ4KdmsIhIEo9Fq/6i6FQMv6sdlnx7kcEiIkk8dkyrsLQGBaXVyCvVI+5gMoNFRJJ4ZE/LYDAiNasctTo1g0VEdpG0p1VZWYm9P15Aela5LJMeSyuGr48PEg8fY7CIyC4qo9Fo9PRKEBFJ5fFTHoiI7MFoEZFQGC0iEgqjRURCYbSISCiMFhEJhdEiIqEwWkQkFEaLiITi0AXTcfv3Ysrkx/DsxNvh52u9e3HH8nAyowzTnnwIX6zd4dBKiiYudi8mPxaDqe17wk9lffscKr+M05Wl+P34h7E69ns3rSE1tHP/j5g0JQaGSUMBPxtviSNngDM5GP1kDPav/dY9K0g3sTtacfv34snHJ2P9wigMv7u91WWXb0pHxm9XMaRfKDp2vM3hlRRJXOxePDF5Cj7vGY37WneyuuyXl3/FuaorGKhpi45h3rF9lGbn/h8R88QUGD56DojsYX3hjXHAhXygfzeEdbT+2pLr2PXx0BSsb94fKilYC1f/iq/n34tBfUKcWklRmIK18o6RkoK1OCcFy7sPx92atm5aQ2rIFKy6D2dJC9bnscD7zwD9urhj9cgCydFyNFhD7/SON6SjwbqnpfVtSa7hcLDu6u6W9SPLJH88/MOsJ9E9XIMV32VgxXcZFperqq7Dr5nlig6WSqW68Wu5vuTiucefQld/DVbnp2F1fprF5ar0dThTWdpkgtVwWwK3bk/T485sZ1e8XhOfnQZjeCiwMb7+P0uqa4Hzl2UPlq3tZu05rtjGIpEcrfD2gRgx0Pbn+IPH8xDZO8SpYDnygto7ttFohEqlumUuR+e8LaAlhgbbPi71S1kuBmhCHQ6WSqW6ad0a/17qGI05uo1Nz5NrOzbmqtcLHYLhM8j2d7kZjmXA2KeLw8GytK1tbTeyTHK0RgzshPkvRtpcbv5K4HhaocMr1PiH1NxjjR+39Gtzj5n74Tb3fHtjMDT4Nrx9+702l1uUmYyT5fmSxzXHtG62to+5P2v8XHP/v5b2bOzZ4zH3F4+l16vxcyw9bm0se18v9aBe8Hl5ou0Fl2+HPvWC5HEtsWfdbL2Oln7WG28zqe8X0Sj6PC1zfyM1/L2tX1sj8otmjqUfWEvRsXcca28Ic8z9BWHp9Wo8vq2gicjaXmLj5QDrr2PDbWvpz+15v4hG0dFyhpQ3r6hvAMDyR2hbH/1EPB4iwjraYmkv39xygHwxETFKtig6WlL/drJ3TMD2MRIlM/e3rLU3trmPEA3HUnIMmsLrZQ+pgba0LRr/uen1VfJrbC+3REtXZ5C8rKU9JGt7TrbGsueN7Im/1XVG6dvHlsb/L7Y+JtraY7O1/Ru/SUy/N/fn1tjaw1DS6wWd3u6nmDv+aOm1ary8rY+M5v688evTlGLv8vseJp4qxje7L2L7rhjJz7H0A2jpoKzUx62N7SnJ1wqwqTQLO6YslbR84x9Ua2G29FxHl5Oyfe0Z29oxL0fHd7mUTKh3JePpnXMkLW7PtpR7PEfeLyJwabQSTxXj+fdPYPO27YgaPtqVUwkp+VoBXstJwpYd3yNqTLSnV8ejLO3xKUpKJnwXbMD2rd9j7PCRnl4br+WyaJmCtWnzVkSPfsBV0wjLFKzvtm1F9HhuH0VGqiFTsDZvwYTR93t6bbyaS45plVypYbCsKNXVMFgiKa9ksBRE8s1aRw3qJPmM+EuF1/H5mk1eFayo4HDJZ8Tn1l7Dl1u+ZbA8SH1Pb6glnhGP/DLsWrOOwVII3mGaiISi6PO0iIgaY7SISCiMFhEJhdEiIqEwWkQkFEaLiITCaBGRUBgtIhIKo0VEQnHogumd+37CpJgYqCJHAz7Wh9BnnQbyf8PoiZOxf/MGh1aSiMjE7mjt3PcTJj8+BT6Pvw51lwiry+qS9gDFl4GwHgjrxDvyEpHz7Pp4aAoWYv4sKViGg1uAiX8CbrvdqZUkIjKRHC2Hg/U721fSExFJJfnj4aSp04HWHaA6sheGI3stLmfU1sJQmM1gEZFLSI6WOqgtfLv3tblcXVYq0Kk7DA4GS4iv3SUij5EcLd/ufdH8/qdsLleNjdDmnHNqpQBpdz6WeoPPhs8xNw4RiUOY87TsvWOuuXu/WRqHiMQhTLRMpEam8R6XufvOEZF4XH7fQ7nZ83HO1t4XEYnHPdHS19n9FHN30TV3y29bz2/I0l2RGTAicbj+42FOBtSnD+LpiQ9JWtx0/KnhcShrj9n6tbmxrM1BRMrm2mjlZMB392fY+f02jB010qVTEZF3cF20/hus7Vs3Y8JY3i+OiOThmmhVXWOwiMglJB+Ir8tKRTU2SloOVRUMFhG5BO8wTURCEe7kUiLybowWEQmF0SIioTBaRCQURouIhMJoEZFQGC0iEgqjRURCYbSISCj/AcOQSxUko2ogAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=301x201 at 0x7FC6F8EE1F70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualkeras.layered_view(cnn_model, legend=True, draw_volume=True, spacing=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Complie a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training a model\n",
    "\n",
    "- Load MNIST data\n",
    "- Traning data (60k) + Testing data (10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "60000 test samples\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_train.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 0.3123 - accuracy: 0.9106 - val_loss: 0.0941 - val_accuracy: 0.9738\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 17s 40ms/step - loss: 0.0858 - accuracy: 0.9733 - val_loss: 0.0608 - val_accuracy: 0.9842\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.0617 - accuracy: 0.9806 - val_loss: 0.0492 - val_accuracy: 0.9867\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0503 - accuracy: 0.9844 - val_loss: 0.0457 - val_accuracy: 0.9877\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 0.0426 - val_accuracy: 0.9863\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 18s 41ms/step - loss: 0.0357 - accuracy: 0.9890 - val_loss: 0.0396 - val_accuracy: 0.9887\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 19s 45ms/step - loss: 0.0308 - accuracy: 0.9904 - val_loss: 0.0344 - val_accuracy: 0.9905\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 0.0394 - val_accuracy: 0.9890\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 16s 38ms/step - loss: 0.0237 - accuracy: 0.9929 - val_loss: 0.0413 - val_accuracy: 0.9880\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 17s 39ms/step - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.0410 - val_accuracy: 0.9892\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 16s 38ms/step - loss: 0.0174 - accuracy: 0.9944 - val_loss: 0.0393 - val_accuracy: 0.9898\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.0150 - accuracy: 0.9956 - val_loss: 0.0408 - val_accuracy: 0.9897\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.0339 - val_accuracy: 0.9910\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 18s 42ms/step - loss: 0.0117 - accuracy: 0.9958 - val_loss: 0.0381 - val_accuracy: 0.9905\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.0102 - accuracy: 0.9963 - val_loss: 0.0506 - val_accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9162c98b0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "cnn_model.fit(X_train, y_train, batch_size=batch_size,\n",
    "          epochs=epochs, verbose=1, validation_split=.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04123755171895027\n",
      "Test accuracy: 0.9883999824523926\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "537eb0b9ab41e35305ac70be8380de5b6e6ad4466bdb4dd5f41276d259677184"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('py4ai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
